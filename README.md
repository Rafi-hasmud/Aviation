# Aviation Data Analysis

- Download Raw Dataset -- https://bit.ly/3mCysVz
- Download Cleaned Dataset - https://bit.ly/3YvEqon

# Introduction

The Aviation Data Analysis project aims to analyze data on flights to identify patterns and trends related to arrival and departure times, delays, cancellations, and other factors that affect the performance of airlines. The project uses a dataset that contains information on flights from various sources, including airlines, airports, and government agencies. 

The project consists of several stages, starting with data cleaning to address missing values, errors, and inconsistencies in the dataset. This is followed by exploratory data analysis to identify patterns and trends in the data, and statistical modeling to test hypotheses and make predictions based on the data.

# Goal

The ultimate goal of the project is to provide insights and recommendations that can help airlines and airports improve their performance and better serve their customers. 

This documentation provides a detailed overview of the Aviation Data Analysis project, including the methodology, data sources, data cleaning process, data analysis techniques, and findings. The documentation is intended to provide a comprehensive guide for anyone interested in replicating or building upon the project's analysis.

I hope that this project will contribute to the ongoing efforts to improve the performance of airlines and airports, and I invite feedback and suggestions for future iterations of the project.

# Files 
The project includes the following files:


# Data Wrangling using Excel

The first step in any data analysis project is to collect and clean the data. In this project, I collected data on aviation incidents from various sources and consolidated them into a single dataset. This dataset contained a large number of missing values, errors, and inconsistencies, which needed to be addressed before any meaningful analysis could be performed.
To clean the data, I used various formulae in Excel to identify and correct errors, inconsistencies, and missing values. I also used conditional formatting and data validation to ensure that the data was consistent and accurate.
The following steps I am taken to clean the dataset:
1.	Identify and remove duplicate records: I used Excel's built-in tools to identify and remove any duplicate records from the dataset. This step helped to reduce the size of the dataset and avoid any errors due to duplicate records.
2.	Handle missing values: I used various formulae to fill in missing values in the dataset. For example, I used the IF function to check if a value was missing and then filled in a default value or a value based on the adjacent data.
3.	Correct errors: I used formulae to correct any errors in the dataset. For example, I used the VLOOKUP function to correct misspelled names or to map one value to another.
4.	Standardize data: I used formulae to standardize data in the dataset. For example, I used the UPPER function to convert all text to uppercase or the PROPER function to capitalize the first letter of each word.
5.	Remove unnecessary columns: I removed any columns from the dataset that Ire not required for the analysis. This step helped to reduce the size of the dataset and make it more manageable.
6.	Validate data: I used data validation to ensure that the data in the dataset was consistent and accurate. For example, I set up data validation rules to ensure that all dates were in the correct format and fell within a specific range.
By following these steps, I was able to clean the dataset and prepare it for analysis. The cleaned dataset was then saved in a separate file to ensure that the original data was not lost or overwritten.

# Data Interpretation, Exploration and Wrangling using SQL



